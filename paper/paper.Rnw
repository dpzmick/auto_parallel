\documentclass[10pt, letterpapper]{proc}
%\documentclass[12pt, letterpapper, titlepage]{article}
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\usetikzlibrary{positioning}

\usepackage{minted}
%\usemintedstyle{emacs}
%\usemintedstyle{borland}
\usemintedstyle{friendly}

\usepackage{subcaption}
\usepackage{caption}

\usepackage[nottoc,numbib]{tocbibind}
\bibliographystyle{plain}

\def\mintcode#1{\mbox{\mintinline{clojure}{#1}}}
\def\code#1{\mbox{\texttt{#1}}}
\def\parlet{\code{parlet}}
\def\parexpr{\code{parexpr}}
\def\defparfun{\code{defparfun}}

\author{
    Zmick, David\\
    \texttt{zmick2@illinois.edu}
}

\title{Macros for straightforward parallelism in Clojure}

\begin{document}
<<include=FALSE>>=
source("../vis/basic_plot.R")
# data <- read.csv("../newnew.csv")
data <- read.csv("../data.csv")
@

\maketitle

% \tableofcontents
% \pagebreak

% question: Can macros be used to create interesting and useful parallelism
% tools for recursive functions in clojure.
\section{Introduction}

Clojure is a lisp--like language running on the Java Virtual Machine\cite{Hickey2008}.
The language's default immutable data structures and sophisticated Software Transactional Memory (STM) system make it well suited for parallel programming\cite{Kraus2009}.
Because Clojure runs on the Java Virtual Machine, Clojure developers can take advantage of existing cross--platform parallelism libraries, such as Java's excellent \code{ExecutorService} framework, to write parallel code.

% for example, writing a parallel map with atoms would be a big pain
However, taking advantage of Clojure's parallel potential is not entirely straightforward.
STM has proven to be very successful construct for concurrent programming\cite{Jones2007a}, but these constructs are often too low level to be of much use to developers whose central concerns are not parallelism\cite{Boehm2009}.

As a result, there are a variety of libraries designed to allow developers to take advantage of the parallelism potential in Clojure.
Clojure built ins such as \code{pmap}\footnotemark[1] and \code{reducers}\footnotemark[2] library provide data parallel sequence manipulation and transformation functions.
Third---party libraries like Tesser\footnotemark[3] and Claypoole\footnotemark[4] provide more data parallel APIs with slightly different goals than the builtin functions.
Developers have a good relationship with data parallel problems\cite{Okur2012a}, but Clojure's nature as a functional language with immutable structures also makes it possible to easily exploit control parallelism (also known as task parallelism\cite{Andradea, Rodr}).

\footnotetext[1]{\url{https://clojuredocs.org/clojure.core/pmap}}
\footnotetext[2]{\url{http://clojure.org/reference/reducers}}
\footnotetext[3]{\url{https://github.com/aphyr/tesser}}
\footnotetext[4]{\url{https://github.com/TheClimateCorporation/claypoole}}

Using Clojure's macro system, I have implemented a set of macros which allow developers to take advantage of Clojure's task parallelism potential.
I have shown that it is possible to attain reasonable degrees of parallelism with minimal modification to existing serial code.

\section{Related Work} \label{label:related_work}
% only discuss task based parallelism in this section
Clojure has access to all of the JVM, so it has access to the Java \code{ForkJoinPool} library\cite{Lea2000}.
The \code{ForkJoinPool} library allows Java programmers to create lightweight, recursive tasks which are executed by a thread pool.
Each thread maintains a queue of tasks to work on.
When a thread runs out of tasks, it steals tasks from other threads.
Work stealing has been used a scheduling mechanism is a variety of modern threading libraries and has been very successful \cite{Lea2000, Blumofe1994, Blumofe1995a}
Users of a \code{ForkJoinPool} create a subclass of \code{RecursiveTask} to compute the value of some function.
Each of these tasks may create more \code{RecursiveTask}s, submit them to the \code{ForkJoinPool}, then wait for their subtasks to complete, without blocking the pool.
Clojure programmers can use the \code{ForkJoinPool} libraries from Clojure, but the interface isn't exactly programmer friendly (or idiomatic).

Clojure also has built in support for task parallelism via \code{future}\footnote{\url{https://clojuredocs.org/clojure.core/future}}.
Each call to \code{future} spawns a thread to result of some function.
When a user is ready to access the computed value, they \code{deref} the future.
\code{deref} will block until the value is computed.
For a user, \code{future} seem like a natural way to parallelize recursive functions.
Unfortunately, \code{future} does not work well when a large number of tasks are created.
Because thread creation overhead is high, users must be careful not to create an excessive number of threads or create threads which do too little work.

Claypoole's implementation of \code{future} offers many improvements to the built in \code{future}.
The Claypoole implementation executes tasks in a fixed sized thread pool, but, the executing thread will block if the task creates a new task, then immediately \code{deref}s it.
This means that a programmer must be very careful not to create tasks in a Claypoole thread pool which depend on other tasks that will also be submitted to the pool, otherwise their code may deadlock.

The work in this paper is conceptually similar, but the interface I have used differs dramatically from the \code{future}s interface.
My code also creates many tasks, but these tasks are created on a Java \code{ForkJoinPool}, so the tasks are much more lightweight, and they can be composed recursively.

In Common Lisp, the \code{lparallel}\footnote{\url{https://lparallel.org/overview/}} library and macros are available.
The macros I have implemented are very similar to the macros \code{lparallel} provides, but, Common Lisp programming conventions are not as ideal for these kinds of macros, and Common Lisp programmers can't use the battle tested \code{ForkJoinPool} implementation.

\subsection{Other languages}
In other languages, frameworks like Cilk++\footnote{\url{https://www.cilkplus.org/}}, OpenMP\footnote{\url{http://openmp.org/wp/}}, Threading Building Blocks\footnote{\url{https://www.threadingbuildingblocks.org/}} have been implemented.
Some of these require compiler support and advanced dependence analysis to guarantee correctness.
The macros I've implemented are an attempt to bring some of the niceties of these libraries and compiler extensions to Clojure, without the need to compiler support or complicated dependence analysis.

\section{Mostly Pure Functions} \label{label:mostly_pure}
Before discussing the macros I've implemented, I need to loosely define a ``mostly pure'' function.
A mostly pure function is a thread---safe function with no side effects directly impacting the values of user-defined values, at the current level of abstraction.
Mostly pure functions can be reordered (or interleaved) without impacting the values of user variables, although the change may impact I/O behavior and output order of a program.
In some cases, the order of certain side effects may not matter to a programmer.
For example, a programmer writing a web scraper may not care what order \mintcode{(download file1)} and \mintcode{(download file2)} execute in, but the order may matter to a programmer writing a I/O constrained server.
When a programmer feels that it may be acceptable to change the order of, or interleave, calls to mostly pure functions, we can reorder them subject to these constraints:

\begin{enumerate}
    \item A call to a mostly pure function $f$ in a block $B$ in a function's control flow graph can safely be moved to a block $P$ for which all paths in the graph though $P$ also go through $B$.
        Figure \ref{fig:move_mostly_pure} provides and example of this constraint.
    \item All of the arguments to the function are available at any block $P$ which is a candidate location for the function call.
        See Figure \ref{fig:move_mostly_pure_through_let} for an example.
\end{enumerate}

The first constraint is introduced so that we avoid network requests or print statements which never would have originally occurred along any given path of execution.
We do not want to allow reordering which introduces new computations or would result in unpredictable performance.
The second constraint ensures that we don't ever violate the most basic of correctness properties.
A more detailed algorithm for finding safe locations for portable mostly pure functions in Clojure code is discussed in Section \ref{label:mpure_algo}

\tikzstyle{goodnode} = [rectangle, draw=black, text width=0.8\linewidth]
\tikzstyle{badnode}  = [rectangle, draw=none,  text width=0.8\linewidth]

\begin{figure}[h]
\centering
\begin{tikzpicture}[node distance=0.62cm,auto]
    \node[coordinate] (0) at (0.7,0) {};
    \node (start0)      [badnode]                                        {\mintcode{;; cannot move the call outside}};
    \node (start)       [badnode, below of=start0]                       {\mintcode{(if (pred? a b c ...)}};
    \node (iftruedo)    [goodnode, below of=start, right of=start]       {\mintcode{(do}};
    \node (iftruelet1)  [goodnode, below of=iftruedo, right of=iftruedo] {\mintcode{;; code that uses a, b, and/or c}};
    \node (iftruelet2)  [goodnode, below of=iftruelet1]                  {\mintcode{(foo a b c))}};
    \node (iffalse)     [badnode, below=2.8cm of 0]                      {\mintcode{(bar a b c))}};
\end{tikzpicture}
\caption{The call to the mostly pure function \code{foo} can only be moved to the boxed nodes}
\label{fig:move_mostly_pure}
\end{figure}

\begin{figure}[h]
\centering
\begin{tikzpicture}[node distance=0.61cm,auto]
    \node[coordinate] (0) at (1.0,0) {};
    \node (a)           [badnode]                          {\mintcode{(let [a (bar 10)]}};
    \node (b)           [goodnode, below of=a, right of=a] {\mintcode{;; code that uses a}};
    \node (c)           [goodnode, below of=b]             {\mintcode{(let [b (bar a)]}};
    \node (d)           [goodnode, below of=c, right of=c] {\mintcode{;; code that uses a and/or b}};
    \node (e)           [goodnode, below of=d]             {\mintcode{(foo a)))}};
\end{tikzpicture}
\caption{The call to the mostly pure function \code{foo} can only be moved to the boxed nodes}
\label{fig:move_mostly_pure_through_let}
\end{figure}

Many Clojure functions fit this definition due to Clojure programming conventions and default immutable data structures.
In this paper, we rely on the judgment of the programmer to tell us when a function can be considered mostly pure.

% A mostly pure function is similar to a \code{const} function in C++.
% \code{const} functions promise not to modify any members of the object on which they are called (unless the member was declared \code{mutable}), essentially promising that the object will not change, from the user of the object's perspective, when the \code{const} function is called.

\begin{figure*}[t!]
\begin{subfigure}[b]{0.5\textwidth}
\centering
\RecustomVerbatimEnvironment{Verbatim}{BVerbatim}{}
\begin{minted}{clojure}
(parlet
  [a (foo value1)
   b (foo value2)]
  ;; some other code here
  (+ a b))
\end{minted}
\subcaption{Example of a parlet form}
\label{fig:parlet_basic}
\end{subfigure}%
\begin{subfigure}[b]{0.5\textwidth}
\centering
\RecustomVerbatimEnvironment{Verbatim}{BVerbatim}{}
\begin{minted}{clojure}
(let
  [a (fork (new-task (fn [] (foo 1))))
   b (fork (new-task (fn [] (foo 2))))]
  ;; some other code here
  (+ (join a) (join b)))
\end{minted}
\subcaption{An expanded parlet form}
\label{fig:parlet_basic_expanded}
\end{subfigure}
\caption{}
\label{fig:parlet_example}
\end{figure*}

\newpage

\section{parlet}
The first of the parallel macros is called \parlet{}.
The \parlet{} macro has exactly the same behavior as Clojure's \code{let}, but it evaluates all of its bindings in parallel.
For example, suppose I had some long running function \code{foo}.
I need to add the result of two calls to this function.
In Figure \ref{fig:parlet_example}, we use \parlet{} to make two calls to \code{foo}, then add the results.

In this example, the expressions \mintcode{(foo value1)} and \mintcode{(foo value2)} are both evaluated as \code{RecursiveTask}s in a \code{ForkJoinPool}.
The calls to \code{foo} are both forked immediately, then we attempt to evaluate the body of the \parlet{}.
Each use of \code{a} and \code{b} is replaced with a call to \code{join}, to get the computed value.
This means that the code in the body of the \code{let} which does not depend on the computations of \code{a} and \code{b} can execute without delay.
Additionally, since the \code{ForkJoinPool} is designed for recursive workloads, tasks which are currently executing can create new tasks, submit them to the pool, then wait for the task to complete, without blocking tasks created by other \parlet{} calls.
This means that a programmer does not have to worry if functions called in the bindings of a \parlet{} form also use \parlet{}.

\subsection{Dependencies}
The \parlet{} macro also supports simple dependency detection.
Clojure \code{let} forms can use names defined previously in the same \code{let}.
The bindings are evaluated from first to last.

\begin{figure}[h]
\centering
\RecustomVerbatimEnvironment{Verbatim}{BVerbatim}{}
\begin{minted}{clojure}
(parlet [a 1
         b (+ 1 a)]
  a)
\end{minted}
\caption{A parlet containing a dependency}
\label{fig:parlet_dep}
\end{figure}

Without the \parlet{}, the let form in Figure \ref{fig:parlet_dep} would evaluate to 2.
If we plan on evaluating each binding in parallel, we can't allow the bindings to have dependencies.
So, the \parlet{} macro will halt the compiler and report an error to the user if any dependencies are found in the \parlet{} form.

\subsection{Correctness}
This transformation is only safe when \code{foo} (and more generally, any function call in the bindings) is a mostly pure function.
If the programmer chooses to use a \parlet{} form, we assume that the functions called in the bindings are mostly pure.
This simple dependency check, as well as the programmers promise that all function called are mostly pure Clojure code, allow us to ensure correct parallelism with this macro.

\section{defparfun}
\begin{figure*}[t!]
\captionsetup[subfigure]{belowskip=20pt}
\begin{subfigure}[b]{0.5\textwidth}
\RecustomVerbatimEnvironment{Verbatim}{BVerbatim}{}
\centering
\begin{minted}{clojure}
(defn f [a]
  (if (pred? ....)
    (do ...)
    (do ... (f (+ 1 a))))))
\end{minted}
\subcaption{}
\label{fig:moveit_a}
\end{subfigure}%
\begin{subfigure}[b]{0.5\textwidth}
\RecustomVerbatimEnvironment{Verbatim}{BVerbatim}{}
\centering
\begin{minted}{clojure}
(defn f [a]
  (if (pred? ....)
    (do ...)
    (let [e1 (f (+ 1 a))]
      (do ... e1))))
\end{minted}
\subcaption{}
\label{fig:moveit_b}
\end{subfigure}
\captionsetup[subfigure]{belowskip=2pt}
\begin{subfigure}[b]{0.5\textwidth}
\RecustomVerbatimEnvironment{Verbatim}{BVerbatim}{}
\centering
\begin{minted}{clojure}
(defn f [...]
  (let [a ...]
    (let [b ...]
      (f (+ 1 a)))))
\end{minted}
\subcaption{}
\label{fig:moveit_c}
\end{subfigure}%
\begin{subfigure}[b]{0.5\textwidth}
\RecustomVerbatimEnvironment{Verbatim}{BVerbatim}{}
\centering
\begin{minted}{clojure}
(defn f [...]
  (let [a ...]
    (let [e1 (f (+ 1 a))]
      (let [b ...] e1)))))
\end{minted}
\subcaption{}
\label{fig:moveit_d}
\end{subfigure}
\caption{Moving a recursive function call}
\label{fig:moveit}
\end{figure*}

\defparfun{} allows a programmer to parallelize calls to a recursive function.
The \defparfun{} macro supports a granularity argument, allowing the programmer to specify when they would like to stop creating additional parallel tasks.
The macro emits the expression provided for granularity inside of an \code{if} statement at the top of the function, so the programmer can use any arbitrary condition, including a condition dependent on the function's arguments, to decide when to stop spawning new tasks.
Figure \ref{fig:fib_parfun} defines a parallel Fibonacci function, which will only execute in parallel when the value of it's argument is greater than $35$.

\subsection{Implementation} \label{label:mpure_algo}
In Clojure, any form which introduces control flow will eventually expand to an \code{if} for.
Any form which introduces new bindings (including Clojure's destructuring mechanisms) will eventually expand to a \code{let} form.
Because the constraints on movement of mostly pure functions only depend on control flow and variable bindings, we only need to make decisions about mostly pure function calls near \code{if} forms and \code{let} forms, in the full expanded versions of Clojure functions.

When provided a function to manipulate, this function first expands all of the other macros in the function body, to get the nice property described above.
Then, the macro recursively crawls the function body, looking for recursive call sites.
If a call site is found, the recursive call (in the function body) is replaced by a newly introduced variable, and we hold onto the original recursive call.
When all of the subexpressions for a given expression have been evaluated and transformed, we check if the expression is a \code{if} form or a \code{let} form.
If we are sitting on a \code{if} form, all of the bindings introduced by the \code{true} branch and by the \code{false} branch are emitted in a new \code{let} expression (Figures \ref{fig:moveit_a} and \ref{fig:moveit_b}).
This guarantees that condition 1 holds (Section \ref{label:mostly_pure}).
If we are sitting on a \code{let} form, any of the bindings which depend on the expressions introduced by the \code{let} form are emitted.
The remaining bindings continue to trickle upwards (see Figures \ref{fig:moveit_c} and \ref{fig:moveit_d}).
This guarantees that condition 2 holds.

After this transformation, it is possible to replace the \code{let} forms which bind the function results to their values with \parlet{} forms providing the same bindings.
The introduction of the \parlet{} form introduces parallelism, so each recursive call will execute in the \code{ForkJoin} pool, in parallel.
To complete the Fibonacci example, the \defparfun{} expansion is shown in Figure \ref{fig:parfun_result}.

Because the transformation does not violate any of the properties defined for mostly pure functions, this transformation is safe when the function being declared as a \code{parfun} is mostly pure.

\begin{figure}[h]
\RecustomVerbatimEnvironment{Verbatim}{BVerbatim}{}
\centering
\begin{minted}{clojure}
(defn fib [n]
 ;; granularity check
 (if (< n 35)
  (if (or (= 0 n) (= 1 n))
   1
   (+ (fib (- n 1))
      (fib (- n 2))))

  ;; recursive case
  (if (or (= 0 n) (= 1 n))
   1
   (parlet [expr17300 (fib (- n 1))
            expr17301 (fib (- n 2))]
    (+ expr17300 expr17301)))))
\end{minted}
\caption{Transformed Fibonacci function}
\label{fig:parfun_result}
\end{figure}

\section{Benchmarking}

\begin{figure*}[t!]
\begin{subfigure}[b]{0.5\textwidth}
\RecustomVerbatimEnvironment{Verbatim}{BVerbatim}{}
\centering
\begin{minted}{clojure}
(defn fib [n]
  (if (or (= 0 n) (= 1 n))
    1
    (+
     (fib (- n 1))
     (fib (- n 2)))))
\end{minted}
\subcaption{Serial Fibonacci function}
\label{fig:fib_serial}
\end{subfigure}%
\begin{subfigure}[b]{0.5\textwidth}
\RecustomVerbatimEnvironment{Verbatim}{BVerbatim}{}
\centering
\begin{minted}{clojure}
(defparfun fib [n] (< n 35)
  (if (or (= 0 n) (= 1 n))
    1
    (+
     (fib (- n 1))
     (fib (- n 2)))))
\end{minted}
\subcaption{Fibonacci function with \defparfun{} added}
\label{fig:fib_parfun}
\end{subfigure}
\caption{}
\label{ref:fib_code}
\end{figure*}

To run benchmarks, I used Google's Cloud Compute virtual machines.
For each trial, a virtual machine was created.
Each virtual machine had either $1$, $2$, $4$, or $6$ cores and $6$ gigabytes of RAM.
First, the serial version of the code was run, then, on the same machine, the parallel version of the code was run.
After both trials finished running, the data was copied back to my local machine and the virtual machine was destroyed.
For every pair of serial/parallel executions, the speedup was computed.
These per--machine speedups are used to generate the plots shown.

To workaround the difficulties JVM benchmarking introduces, the Criterium\footnote{\url{https://github.com/hugoduncan/criterium}} library was used for Clojure code and the Caliper\footnote{\url{https://github.com/google/caliper}} library was used for Java code.
Because each benchmark was run on it's own virtual machine with a constrained number of cores, the number of threads the JVM could create was controlled (including threads used for garbage collection)

\subsection{Fibonacci}

First we will look at the classical recursive Fibonacci example.
Figure \ref{fig:fib_serial} shows the serial benchmark code; \ref{fig:fib_parfun} shows the code parallel benchmark code.
In Figure \ref{fig:cljfib} the results from many trials of this code running $1$, $2$, $4$, and $6$ cores.
Each benchmark computes \mintcode{(fib 39)}.
We see that we get about a 3x speedup with $6$ cores.
This speedup isn't quite what we would hope to see, but, as can be seen in Figure \ref{fig:javafib}, the handwritten Java \code{ForkJoinPool} implementation gets about the same speedup with $6$ cores on these virtual machines.
Previous \code{ForkJoinPool} benchmarks have shown much better speedups for similar code\cite{Lea2000}, so I suspect that the virtual machine configuration is somewhat responsible for the discrepancy in results.

<<cljfib, fig.pos="h", fig.cap="Fibonacci Clojure (defparfun) Performance", echo=F, out.width='\\linewidth', fig.align='center'>>=
cljfib <- plot_means_error_for_benchmark(data, "fib", "parfun")
@

<<javafib, fig.pos="h", fig.cap="Fibonacci Java Performance", echo=F, out.width='\\linewidth', fig.align='center'>>=
jfib <- plot_means_error_for_benchmark(data, "fib", "fj")
@

The large variance we see in the Clojure benchmarks is somewhat disturbing, especially since it does not show up in the Java results.
Since it does not appear in the Java benchmarks, it is not a result of variable performance in the Google Cloud Platform virtual machines.
In Figure \ref{fig:fib_sd} I show the standard deviation of the mean runtime for the serial and parallel Clojure Fibonacci functions, along with their Java counterparts.
Notice that the Java results do not have nearly as high deviations from mean runtime.
While I cannot completely explain the variability, it seems to be caused by the increased pressure Clojure's function implementation and the \code{ForkJoinPool} wrapper tasks places on the JVM garbage collector.
Every Clojure function is an \code{Object} created following the Clojure \code{IFn} interface\footnote{\url{http://clojure.org/reference/special_forms\#fn}}.
When running on the \code{ForkJoinPool}, each function is further wrapped in a \code{RecursiveTask} object, causing additional allocations.
This effectively moves the stack for the recursive function to the heap (eliminating Clojure's stack depth limit\footnote{\url{http://clojure.org/about/functional\_programming\#\_recursive\_looping}}).
The garbage collector behaves somewhat non---deterministically, so I believe this is the explanation for the large variation in runtime for the serial and recursive Fibonacci code.
We can avoid excessive task creation by controlling the granularity of parallelism, to an extent.
The Fibonacci example highlights this problem because the function call overhead greatly exceeds the amount of work each call is doing.
We will see an example for which this is not the case in Section \ref{label:id3_bench}.

<<fib_sd, fig.pos="h", fig.cap="Fibonacci standard deviations", echo=F, out.width='\\linewidth', fig.align='center'>>=
fib <- data[data$spec.name %like% "fib.big",]

important <- c("cores", "mean.runtime")

clj_serial <- ddply(fib[fib$spec.name %like% "serial",important], "cores", colwise(sd))
clj_parfun <- ddply(fib[fib$spec.name %like% "parfun",important], "cores", colwise(sd))

java_serial <-ddply(data[data$spec.name %like% "serial_java", important], "cores", colwise(sd))
java_par    <-ddply(data[data$spec.name %like% "fj_java", important], "cores", colwise(sd))

# d <- merge(clj_serial, clj_parfun, "cores", suffixes = c("serial", "parfun"))
d <- join_all(list(clj_serial, clj_parfun, java_serial, java_par), by="cores")

nnames <- c("Clojure serial", "Clojure parallel w/ defparfun", "java serial", "java fork/join")
names(d)[2] <- nnames[1]
names(d)[3] <- nnames[2]
names(d)[4] <- nnames[3]
names(d)[5] <- nnames[4]

barplot(t(as.matrix(d[,nnames])), beside=TRUE, names=d$cores, legend=nnames,
        xlab="cores", ylab="mean runtime standard deviation") #, main = "Fibonacci standard deviations")
@

\subsection{ID3} \label{label:id3_bench}
I also implemented a simple ID3\footnote{\url{https://en.wikipedia.org/wiki/ID3_algorithm}} classifier in Clojure.
The code is bit longer, so it is not included in this paper, but it can be found on this project's GitHub page\footnote{\url{https://github.com/dpzmick/auto_parallel}}.
For each benchmark, a random 1,000 element dataset was created.
Each element of the dataset was given 100 random features.
The \code{ID3} algorithm implementation ran until it was out of attributes to pivot on.

The \code{ID3} code does much more work in each function call, so the overhead introduced by the \code{ForkJoinPool} wrapper does not impact the results as much as it does in the Fibonacci benchmark.
Figure \ref{fig:id3parfun} shows that we get the 3x speedup we expect on these virtual machines with the \code{ID3} algorithm.

<<id3parfun, fig.pos="h", fig.cap="id3 Clojure (defparfun) Performance", echo=F, out.width='\\linewidth', fig.align='center'>>=
id3 <- plot_means_error_for_benchmark(data, "id3", "parfun")
@

\section{Conclusions and Future Work}
The Clojure macros I've implemented perform transformation which can speedup Clojure code to a degree which matches the speedups attained using handwritten Java code, running on the same hardware.
Parallelism is difficult, and automatic parallelism is possible\cite{Banerjee1993}, but these techniques are complicated and often do not get the desired results and the research community has begun to feel the need for explicit parallelism in programs.\cite{Arvind2010}
Languages like Clojure are well suited for this parallelism and techniques like mine can easily be implemented.
In a language with a strong STM system and immutable structures, such transformations are easy to reason about, making it much simpler for programmers to implement explicitly parallel programs.

Macros of this style do not inhibit the programmers ability to use the other mechanisms implemented in the language, although interoperability with them could be improved.
For example, one of the tests which was not discussed in this paper involved using the STM system from within a function declared with \defparfun{}.
Benchmarks on this code behaved correctly and performance improved as expected.
However, if a programmer attempted to use a \code{pmap} or \code{future} inside of a \defparfun{} or \parlet{}, the two systems would create separate thread pools and the number of created threads would be large, possibly causing poor performance.
There are also a variety of other useful macros in lparallel\footnote{\url{https://lparallel.org/}} that may be useful to implement in Clojure and would complement the macros I've implemented in this project.

% It would also be useful to use profiling tools and implement static analysis tools which would detect potential locations for these macros.
% These tools could be used by developers to look for potential opportunities for parallelism, and they could be used to demonstrate the claim that many Clojure programs may benefit from use of these macros.

% The intended users of these macros are developers whose primary concern is not performance, but may benefit from a simple mechanism with which they can take advantage of the many cores in their machines.
% Developers who are extremely concerned with performance and want a high degree of control should turn elsewhere (perhaps even to Java) to write their highly tuned code.

\bibliography{/home/dpzmick/Documents/Bibtex/senior-thesis.bib}

\end{document}
